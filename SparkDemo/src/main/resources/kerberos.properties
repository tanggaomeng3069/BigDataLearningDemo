# Kerberos认证
kerberos.user.name=zhengzhou
# IDEA本地Local 测试使用
c1.java.security.krb5.conf=D:/code_java/BigDataLearningDemo/SparkDemo/src/main/resources/cluster1/krb5.conf
c1.java.security.auth.login.config=D:/code_java/BigDataLearningDemo/SparkDemo/src/main/resources/cluster1/security_client_jaas.conf
#c1.kerberos.key.path=D:/code_java/BigDataLearningDemo/SparkDemo/src/main/resources/cluster1/zhengzhou.keytab
# 集群Yarn Cluster运行配置
#c1.java.security.krb5.conf=/etc/krb5.conf
#c1.java.security.auth.login.config=/usr/hdp/3.0.1.0-187/kafka/conf/security_client_jaas.conf
#kerberos.key.path=/etc/security/keytabs/zhengzhou.keytab
# 配置 Kafka
# 指定kafka的broker地址，如果有多个broker可以用逗号分隔；
c1.kafka.broker.list=manager.insight.com:6667,master.insight.com:6667,worker.insight.com:6667
c2.kafka.broker.list=manager.inspur.com:6667,master.inspur.com:6667,worker.inspur.com:6667
# 指定kafka组名
c1.kafka.group.id=zhengzhouGroup
# 指定Topic
c1.kafka.topic.read=c1_input
c2.kafka.topic.write=c2_output
# ---------------------------------------------------------------------------------------
java.security.krb5.conf=D:/code_java/BigDataLearningDemo/SparkDemo/src/main/resources/cluster1/krb5.conf
java.security.auth.login.config=D:/code_java/BigDataLearningDemo/SparkDemo/src/main/resources/cluster1/security_client_jaas.conf
kerberos.key.path=D:/code_java/BigDataLearningDemo/SparkDemo/src/main/resources/cluster1/zhengzhou.keytab

# 配置 hdfs，指定hdfs的输入路径
input.path=hdfs://manager.insight.com:8020/tmp/word.txt
# 指定hdfs的写入路径
output.path=hdfs://manager.insight.com:8020/tmp/output
hbase.write.hdfs.path=hdfs://manager.insight.com:8020/tmp/hbasewritehdfs
hdfs.write.hbase.path=hdfs://manager.insight.com:8020/tmp/hdfswritehbase.txt
hdfs.write.hive.path=hdfs://manager.insight.com:8020/tmp/hdfswritehbase.txt
# 数据格式：1,Ambari,V2.7.3,CentOS,7.4.1708,bigdata@ambari.com

# 配置 Kafka
# 指定kafka的broker地址，如果有多个broker可以用逗号分隔；
kafka.broker.list=manager.insight.com:6667,master.insight.com:6667,worker.insight.com:6667
# 指定kafka组名
kafka.group.id=zhengzhouGroup
# 指定Topic
kafka.topic.read=c1_input
kafka.topic.write=c1_output

# 配置 hbase
# 先执行写程序，再执行读程序，程序运行之前，需要先创建HBase数据表（手动创建）
# 创建namespace
# create_namespace 'Test'
# 创建hbase namespace，表
# create 'Test:spark_write', 'info'
# 查看表结构
# desc 'Test:spark_write'
# 查看表中数据
# scan 'Test:spark_write'
# 统计表中数据
# count 'Test:spark_write'
# 删除表
# disable 'Test:spark_write'
# drop 'Test:spark_write'
# HBase表名称(namespace:hbase_table)
hbase.table.name=Test:spark_write
# HBase表列族
hbase.table.column.family=info
# 配置HBase连接zk信息及端口，逗号分隔
hbase.zookeeper.list=manager.insight.com,master.insight.com,worker.insight.com
hbase.zookeeper.port=2181
# HBase开启kerberos之后，可以指定master地址，最好使用以下配置_HOST
hbase.master.kerberos.principal=hbase/_HOST@INSIGHT
# 此处必须写成_HOST，因为regionserver是存在多个节点
hbase.regionserver.kerberos.principal=hbase/_HOST@INSIGHT
hbase.client.retries.number=5
# Insight平台查看hbase的配置参数
zookeeper.znode.parent=/hbase-secure

# 配置 hive，在导入hive-site.xml时，从集群 /usr/hdp/3.0.1.0-187/spark2/conf/hive-site.xml
# 导入到resource目录
# 先手动创建Hive数据库
# create database if not exists test;
# Hive创建表
# create table test.spark_write(active string, name string, version string, os string, Core string, mail string) row format delimited fields terminated by "," STORED AS PARQUET;
hive.db.name=test
hive.table.name=test.spark_write

# 配置 janusGraph
janusgraph.gremlin.server=manager93.bigdata
janusgraph.gremlin.port=8182

# 配置 Elasticsearch
es.nodes=https://manager93.bigdata,https://master94.bigdata,https://worker92.bigdata
#es.nodes=https://manager93.bigdata
es.port=9200
es.nodes.wan.only=true
es.query.index=twitter
es.query=?q=*

es.net.http.auth.user=elastic
es.net.http.auth.pass=123456

# 配置Spark-JanusGraph
storage.hbase.table=janusgraph-test1
storage.hostname=manager93.bigdata:2181,master94.bigdata:2181,worker92.bigdata:2181
storage.zk.parent=/hbase-secure
janus.test.data.path=hdfs://manager93.bigdata:8020/user/hdfs/test-data.txt

hbase.regionserver.keytab.file=/etc/security/keytabs/hbase.service.keytab
janus.java.security.krb5.conf=/etc/krb5.conf

janus.hbase.username=hbase-sanzu93@INSIGHT
janus.hbase.keytab=/etc/security/keytabs/hbase.headless.keytab










